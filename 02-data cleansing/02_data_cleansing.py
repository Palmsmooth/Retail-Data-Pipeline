# -*- coding: utf-8 -*-
"""02-data cleansing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zKoBPXoQhPtY3_pCIN5BiGTEbi3k_VLC

# Data Cleansing

# Step 1: Install Spark and PySpark
"""

!apt-get update                                                                         # Update all packages in this VM
!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                 # Install Java Development Kit (necessary for Spark installation)
!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz  # Install Spark 3.5.1
!tar xzvf spark-3.5.1-bin-hadoop3.tgz                                                   # Unzip the Spark 3.5.1 file
!pip install -q findspark                                                               # Install the Python package for connecting to Spark

# Set environment variables so that Python can recognize Spark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"
os.environ["PATH"] += ":/content/spark-3.5.1-bin-hadoop3/bin"

# Install PySpark version 3.5.1 in Python
!pip install pyspark==3.5.1

# How many cores does the Google Colab server have?
!cat /proc/cpuinfo

# Create a Spark Session to use Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Road to Data Engineer 3.0 App") \
    .getOrCreate()

# Check Python version
import sys
sys.version_info

# Check Spark version
spark.version

"""## Read Data
Download the Parquet file from the results of Workshop 1 (with slight modifications added so you can practice data cleaning in this Workshop 2).
"""

!wget https://file.designil.com/f/6BamyF+ -O w2_input.parquet #-O to rename the file

dt = spark.read.parquet('w2_input.parquet')

dt

dt.show(10)

"""# Step 2: Data Profiling"""

# Check which columns are available
dt

# View the data
dt.show()

# View the first 100 rows of data
dt.show(100)

# View the first 100 rows of data without ellipsis (…)
dt.show(100, truncate = False)

# Check the data type of each column
dt.dtypes

# Another command to view each column’s information (Schema)
dt.printSchema()

"""nullable means that the value can be null (empty)"""

# Count the number of rows and columns
print((dt.count(), len(dt.columns)))

dt.columns

# Summarize statistical information
dt.describe().show()

# Another command to summarize statistical information
dt.summary().show()

# Summarize statistics for specific columns only
dt.select("price").describe().show()

"""## Exercise 1
The boss wants to know which columns have missing values and to display the rows that contain missing values.
"""

# Find which columns have missing values
dt.summary("count").show()

# Display rows that have missing values
dt.where( dt.customer_id.isNull() ).show()

"""## YData Profiling"""

!pip install ydata-profiling

from ydata_profiling import ProfileReport
profile = ProfileReport(dt.toPandas(), title="Profiling Report")

profile

"""# Step 3: EDA - Exploratory Data Analysis

We will try performing Exploratory Data Analysis (EDA) using both non-graphical and graphical

##1) Non-Graphical EDA

We can use Spark commands to retrieve the data we need
"""

#Numerical data
dt.where(dt.price >= 100).show()

#Text data
dt.where(dt.customer_country == 'Australia').show()

"""##Exercise 2:
Q1: How many rows of purchases occurred in May 2024?

Q2: How many rows of purchases occurred in June 2024?
"""

# Q1
dt.where(dt.date.startswith("2024-05")).count()

# Q2
dt.where(dt.date.startswith("2024-06")).count()

"""## 2) Graphical EDA

Spark was not designed for data plotting. Therefore, we will use pandas together with the seaborn and matplotlib packages to plot the data instead.

We will get to know:

1.Boxplot

2.Histogram

3.Scatterplot
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Convert Spark DataFrame to Pandas DataFrame
dt_pd = dt.toPandas()

# View a sample of the data
dt_pd.head()

#Plot 1) Boxplot – Show the distribution of numerical data
sns.boxplot(x = dt_pd['price'])

# Plot 2) Histogram – Show the distribution of numerical data
# bins = number of bars to display
sns.histplot(dt_pd['price'], bins=10)

# Increase bins to show more detailed data
sns.histplot(dt_pd['price'], bins=100)

"""###Exercise 3:

The above plot is hard to read. How can we zoom in to see the values more clearly?
"""

sns.histplot(dt_pd[dt_pd['price']<40]['price'], bins=50)

# Plot 3) Scatterplot – Show the relationship between two variables
# For example, if we want to see how Quantity and Price are related
sns.scatterplot(x=dt_pd.quantity, y=dt_pd.price)

"""# Step 4: Data Cleansing with PySpark

Check for 5 common types of anomalies in the data.
- Data Type
- Syntactical Anomalies
- Semantic Anomalies
- Missing Values
- Outliers

##Data Type

One of the most common problems in data is data types not matching what we need.
"""

# Show data
dt.show(truncate=False)

# Show schema
dt.printSchema()

"""We can see that the date column is read as a string, but we want it to be in date-time format. How can we do that?

First, we need to check what the date column looks like.
"""

dt.select("date").show(10)

"""We will use the to_timestamp function from pyspark.sql.functions"""

# Convert string to datetime
from pyspark.sql import functions as f

# Create a new column named "date"
dt_clean = dt.withColumn(
    "date",
    f.to_timestamp(dt.date, 'yyyy-MM-dd')
)

dt_clean.show()

# Tip: If using a standard date format, you don’t need to specify the date format when using to_timestamp
dt_clean = dt.withColumn(
    "date",
    f.to_timestamp(dt.date)
)

dt_clean.show()

# Check if the data is complete by looking at the min and max of the date column
dt_clean.select(
    f.min(dt_clean.date),
    f.max(dt_clean.date)
).show()

dt_clean.printSchema()

"""Example of how to use Datetime data effectively"""

# Count transactions during the first half of January 2024
dt_clean.where(
    (f.dayofmonth(dt_clean.date) <= 15) &
    (f.month(dt_clean.date) == 1) &
    (f.year(dt_clean.date) == 2024)
).count()

# Tip: Sometimes we don’t need to convert the data type, because Spark is smart enough to automatically convert String -> Timestamp
dt.where(
    (f.dayofmonth(dt.date) <= 15) &
    (f.month(dt.date) == 1) &
    (f.year(dt.date) == 2024)
).count()

"""## Anomalies Check

Use Spark to detect anomalies in the data.

### Anomaly 1) Syntactical Anomalies

Lexical errors, such as misspellings.

#### Exercise 4
Find country names with misspellings, and correct the misspelled names.

Do you see any misspelled country names in the Country column?
"""

# How many countries are included in this dataset?
dt_clean.select("customer_country").distinct().count()

# .sort() = Sort the data alphabetically for easier reading
# .show() = Display the data; if no number is specified, it shows only 20 rows by default
dt_clean.select("customer_country").distinct().sort("customer_country").show( 39 )

"""Did you find any unusual country names?

Let's see what the data for the misspelled countries looks like.
"""

dt_clean.where(dt_clean["customer_country"] == 'Japane').show()

"""It's time to try correcting the misspelled country names."""

from pyspark.sql.functions import when

dt_clean_country = dt_clean.withColumn("customer_country_update", when(dt_clean['customer_country'] == 'Japane', 'Japan').otherwise(dt_clean['customer_country']))

"""Check the corrected data."""

dt_clean_country.select("customer_country_update").distinct().sort("customer_country_update").show(39)

# View what the data looks like now
dt_clean_country.show()

"""Remove the old country column and replace it with the new corrected country column.

"""

dt_clean_v2 = dt_clean_country.drop("customer_country").withColumnRenamed('customer_country_update', 'customer_country')

dt_clean.where(dt_clean["customer_country"] == 'Japane').show()

dt_clean[dt_clean.customer_country == 'Japane'].show()

# View the data (randomly select one Transaction_ID)
dt_clean_v2[dt_clean_v2.transaction_id == '566623'].show()

"""### Anomaly 2) Semantic Anomalies

Integrity constraints: Values that fall outside the acceptable range, for example, a product_id that is longer than it should be.
"""

# Check what the product_id data looks like now
dt_clean_v2.select("product_id").show(100)

# Count the total number of product_id entries
dt_clean_v2.select("product_id").count()

"""#### Exercise 5
The team said to replace any product_id longer than 5 characters so that all of them are exactly 5 characters.

##### Solution
1. Check whether all product_ids follow the desired format.

Hint: Use the website https://www.regex101.com
 to create a Regular Expression for the format you want.
"""

#Check whether all product_ids consist of exactly 5 characters.
dt_clean_v2.where(dt_clean_v2["product_id"].rlike("^[0-9]{5}$")).count() / dt_clean_v2.count()

"""This means that some of the data (10%) does not follow the desired format."""

dt_clean_v2.where(dt_clean_v2["product_id"].rlike("^.{5}$")).count() / dt_clean_v2.count()  #"^.{5}$" – Any 5 characters can be used; the dot (.) matches any character

"""2. Let's look at the incorrect data to see what it looks like."""

dt_correct_product = dt_clean_v2.filter(dt_clean_v2["product_id"].rlike("^.{5}$"))
dt_incorrect_product = dt_clean_v2.subtract(dt_correct_product)

dt_incorrect_product.show(10)

# Check a bit further to see what characters appear after the product_id (randomly select a product_id)
dt_clean_v2.where(dt_clean_v2['product_id'].startswith('15044')).select('product_id', 'product_name').show(truncate=False)

"""This tells us that… the character after the 5-digit product_id indicates the product variation.

For example, products with multiple colors (as in the example above: A = "Pink", C = "Purple", D = "Red").

Therefore, we can remove the extra character in product_id (as the team wants).

3. Let's replace the product_id values
"""

dt_clean_v3 = dt_clean_v2.withColumn('product_id', f.substring('product_id',1,5))

#Check the results
dt_correct_product = dt_clean_v3.filter(dt_clean_v3["product_id"].rlike("^.{5}$"))
dt_incorrect_product = dt_clean_v3.subtract(dt_correct_product)

dt_incorrect_product.show(10)

"""All done! There are no more product_ids in the wrong format.

### Anomaly 3) Missing Values
Checking and handling missing values (if necessary)

A missing value is a value that is empty or null.

How can we find out how many missing values exist in each column?

Method 1 to check for Missing Values

Use the List Comprehension technique – you can review this in Week 1 under Basic Python.

For example: [print(i) for i in [1, 2, 3]]
"""

# col = Spark function to select a column
# sum = Spark function to calculate the sum
from pyspark.sql.functions import col, sum

dt_nulllist = dt_clean_v3.select([
    sum(col(colname).isNull().cast("int")).alias(colname)
    for colname in dt_clean_v3.columns
])
dt_nulllist.show()

"""Method 2 to check Missing Values – Code from Exercise 1

This code is much cleaner than the one above, but you need to do the addition/subtraction manually.
"""

dt_clean_v3.summary("count").show()

# Check which rows have a missing customer_id (same code as Exercise 1)
dt_clean_v3.where(dt_clean_v3.customer_id.isNull()).show()

"""What should the customer_id data normally look like?"""

dt_clean_v3.select('customer_id').distinct().show()

"""####Exercise 6:

The Data Analyst team has requested that we replace any NULL customer_id values with 00000.
"""

# Code to replace NULL values with 00000
dt_clean_v4 = dt_clean_v3.withColumn("customer_id", when(dt_clean_v3['customer_id'].isNull(), '00000.0').otherwise(dt_clean_v3['customer_id']))

#Check whether the user IDs that were NULL have been successfully replaced.
dt_clean_v4.where( dt_clean_v4.customer_id.isNull() ).show()

"""###Anomaly 4) Outliers

Data that is unusually high or low compared to most of the dataset.

Let's use a Boxplot to detect outliers in product prices.
"""

sns.boxplot(x = dt_clean_v4.toPandas()['price'])

"""We can see that some products are priced much higher than most of the data.

Let's check which products have a price over 600.
"""

dt_clean_v4.where( dt_clean_v4.price > 600 ).select("product_id", "product_name", "price").distinct().show(truncate = False)

"""Try searching the product name on Google to see if it’s really expensive.

It turns out to be a storage cabinet — a large product with a vintage (old-cool) style. So it’s not surprising that this item is expensive.

In this case, it is indeed an outlier, but it is not incorrect data, so there’s no need to fix anything.

### Let's try cleaning the data using Spark SQL
"""

# Convert the Spark DataFrame into a TempView first
dt.createOrReplaceTempView("data")
dt_sql = spark.sql("SELECT * FROM data")
dt_sql.show()

# Try converting the code for listing country names from Exercise 4 into SQL
dt_sql_country = spark.sql("""
SELECT DISTINCT customer_country
FROM data
ORDER BY customer_country
""")
dt_sql_country.show(100)

# Try converting the code for replacing country names from Exercise 4 into SQL
dt_sql_transform = spark.sql("""
SELECT
    transaction_id,
    date,
    product_id,
    price,
    quantity,
    customer_id,
    product_name,
    CASE WHEN customer_country = 'Japane' THEN 'Japan' ELSE customer_country END AS customer_country,
    customer_name,
    total_amount,
    thb_amount
FROM
    data
""")
dt_sql_transform.show()

# Check whether the results are correct
dt_sql_transform.select("customer_country").distinct().sort("customer_country").show(50)

"""####Exercise 7
Do Exercise 5 using SQL instead of Python.
"""

#Check whether there are any product_ids that are not 5-character alphanumeric strings.
dt_sql_check_productid = spark.sql("""
SELECT *
FROM data
WHERE product_id NOT RLIKE "^.{5}$"

""").show()

#Replace product_id with its first 5 characters.

dt_sql_productid_clean = spark.sql("""
SELECT
    transaction_id,
    CASE
      WHEN length (product_id) > 5
      THEN substr (product_id,1,5)
      ELSE product_id
    END AS product_id,
    price,
    quantity,
    customer_id,
    product_name,
    customer_country,
    customer_name,
    total_amount,
    thb_amount
FROM
    data
""")
dt_sql_productid_clean.show()

# Check whether the data matches the desired format
# Tip: ~ = NOT (makes the condition the opposite)

dt_sql_productid_clean.filter(~dt_sql_productid_clean["product_id"].rlike("^.{5}$")).show()

"""# Step 5: Data Export in PySpark

##Parquet Export

Save as a Parquet file — a lightweight file format that also stores the data type of each column.
"""

dt_clean_v4.write.parquet("cleaned_data_output2.parquet")

"""Try reading the Parquet file you just saved to check that the file works correctly."""

dt2 = spark.read.parquet("cleaned_data_output2.parquet")

dt2.show(10)

"""Try writing the file with the same name."""

dt_clean.write.parquet("cleaned_data_output.parquet")
# dt_clean.write.mode("overwrite").parquet("cleaned_data_output.parquet")

dt_clean.write.mode("overwrite").parquet("cleaned_data_output.parquet")

"""## CSV Export
If you’re working with a team that needs CSV tables, we can export directly from Spark.

"""

# Save as CSV
dt_clean.write.csv('cleaned_data.csv', header=True)

"""##Excel Export

But if the team wants an Excel file, we can use Pandas.
"""

dt_clean.toPandas().to_excel("output.xlsx")